{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eab7e1a2-d026-41bb-aa93-2b061f12e12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linux rajesh 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux\n"
     ]
    }
   ],
   "source": [
    "!uname -a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e915a32e-9f29-4711-bf1e-eb46c597ed9c",
   "metadata": {},
   "source": [
    "### Streaming Bitcoin Data from Socket to HDFS\n",
    "\n",
    "This code is used to stream real-time Bitcoin price data from a socket and perform windowed aggregation in Apache Spark. The processed data is then written to HDFS in Parquet format.\n",
    "\n",
    "#### Steps:\n",
    "1. **Initialize Spark Session**: A Spark session is created to run the streaming job. The `appName` for the Spark job is set to `\"BitcoinSocketStream\"`.\n",
    "   \n",
    "2. **Define Schema**: The schema for the incoming JSON data is defined, which contains two fields:\n",
    "   - `timestamp`: The timestamp of the data in string format.\n",
    "   - `price`: The Bitcoin price in USD (double type).\n",
    "\n",
    "3. **Read Streaming Data from Socket**: Data is read from a socket source, where the host is `localhost` and the port is `5002`. The incoming data is assumed to be in JSON format.\n",
    "\n",
    "4. **Parse JSON Data**: The `from_json` function is used to parse the incoming stream of JSON data into a structured format, which is then split into individual columns.\n",
    "\n",
    "5. **Data Filtering**: The data is filtered to exclude records where the `price` is `null`.\n",
    "\n",
    "6. **Timestamp Conversion**: The `timestamp` field is cast to a `timestamp` type, which allows it to be used for time-based operations.\n",
    "\n",
    "7. **Windowed Aggregation**: A watermark of `1 minute` is applied to the `timestamp` column to allow Spark to handle late-arriving data. The data is then aggregated in 1-minute windows, calculating the average price in each window.\n",
    "\n",
    "8. **Write Stream to HDFS**: The aggregated data is written to HDFS in Parquet format, with the path specified as `hdfs://localhost:9000/user/rajesh/bitcoin_data`. A checkpoint directory is used to store the state of the streaming query to ensure fault tolerance.\n",
    "\n",
    "#### Output:\n",
    "The data is stored in Parquet format in the specified HDFS path, with each file containing the average Bitcoin price per minute. A checkpoint is also maintained for the stream.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a9db78f-65a7-4bf7-b4ae-8ccd80fe4199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/01 09:49:51 WARN Utils: Your hostname, rajesh resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/05/01 09:49:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/01 09:49:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/01 09:49:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession for streaming (local[*] uses all local cores)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BitcoinStreaming\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7c8f34d-7031-4904-af5a-b53d643e30c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, window, avg\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType\n",
    "from pyspark.sql.functions import window, avg\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BitcoinSocketStream\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for JSON data\n",
    "schema = StructType() \\\n",
    "    .add(\"timestamp\", StringType()) \\\n",
    "    .add(\"price\", DoubleType())\n",
    "\n",
    "# Read stream from socket\n",
    "lines = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 5002) \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON\n",
    "json_df = lines.select(from_json(col(\"value\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Filter out bad data (i.e., where price is null)\n",
    "filtered_df = json_df.filter(col(\"price\").isNotNull())\n",
    "\n",
    "# Convert timestamp to timestamp type\n",
    "filtered_df = filtered_df.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "from pyspark.sql.functions import window, avg\n",
    "\n",
    "# Add watermark before aggregation\n",
    "aggregated_df = filtered_df \\\n",
    "    .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "    .groupBy(window(col(\"timestamp\"), \"1 minute\")) \\\n",
    "    .agg(avg(\"price\").alias(\"avg_price\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Output to HDFS in Parquet format\n",
    "query = aggregated_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"hdfs://localhost:9000/user/rajesh/bitcoin_data\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:9000/user/rajesh/checkpoint\") \\\n",
    "    .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701c59a7-c99f-4ff8-9330-b60fbc4bc0e4",
   "metadata": {},
   "source": [
    "# Inspecting Bitcoin Data from HDFS\n",
    "\n",
    "This notebook demonstrates how to load Bitcoin data from HDFS and inspect the data by extracting and displaying the full timestamp information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73dac05a-a5f5-4c79-8ef5-9304540f05a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- avg_price: double (nullable = true)\n",
      "\n",
      "+--------------------+---------+\n",
      "|              window|avg_price|\n",
      "+--------------------+---------+\n",
      "|{2025-05-01 20:07...|  96526.0|\n",
      "|{2025-05-01 20:08...|  96529.6|\n",
      "|{2025-05-01 20:09...|  96544.0|\n",
      "|{2025-05-01 20:10...|  96536.0|\n",
      "|{2025-05-01 20:11...|  96529.2|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Parquet data from HDFS\n",
    "df = spark.read.parquet(\"hdfs://localhost:9000/user/rajesh/bitcoin_data\")\n",
    "\n",
    "# Show schema and data for inspection\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c2c9603-c8b1-4fd0-aee9-a1cd42116217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|datetime           |avg_price|\n",
      "+-------------------+---------+\n",
      "|2025-05-01 20:07:00|96526.0  |\n",
      "|2025-05-01 20:08:00|96529.6  |\n",
      "|2025-05-01 20:09:00|96544.0  |\n",
      "|2025-05-01 20:10:00|96536.0  |\n",
      "|2025-05-01 20:11:00|96529.2  |\n",
      "|2025-05-01 20:14:00|96562.0  |\n",
      "|2025-05-01 20:15:00|96604.0  |\n",
      "|2025-05-01 20:16:00|96607.0  |\n",
      "|2025-05-01 20:17:00|96607.0  |\n",
      "|2025-05-01 20:18:00|96627.8  |\n",
      "|2025-05-01 20:19:00|96625.0  |\n",
      "|2025-05-01 20:20:00|96625.0  |\n",
      "|2025-05-01 21:28:00|97106.0  |\n",
      "|2025-05-01 21:29:00|97106.0  |\n",
      "|2025-05-01 21:30:00|97093.0  |\n",
      "|2025-05-01 21:31:00|97061.2  |\n",
      "|2025-05-01 21:32:00|97054.0  |\n",
      "|2025-05-01 21:33:00|97027.0  |\n",
      "|2025-05-01 21:34:00|97007.0  |\n",
      "|2025-05-01 21:35:00|97007.0  |\n",
      "+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"InspectBitcoinData\").getOrCreate()\n",
    "\n",
    "df = spark.read.parquet(\"hdfs://localhost:9000/user/rajesh/bitcoin_data\")\n",
    "\n",
    "# Extract the start timestamp from the window struct\n",
    "df_with_full_time = df.withColumn(\"datetime\", col(\"window.start\"))\n",
    "\n",
    "# Show the results with the full time\n",
    "df_with_full_time.select(\"datetime\", \"avg_price\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a8da044-e681-44d7-83db-18b2f3fece59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 658:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|timestamp          |avg_price|\n",
      "+-------------------+---------+\n",
      "|2025-05-01 20:07:00|96526.0  |\n",
      "|2025-05-01 20:08:00|96529.6  |\n",
      "|2025-05-01 20:09:00|96544.0  |\n",
      "|2025-05-01 20:10:00|96536.0  |\n",
      "|2025-05-01 20:11:00|96529.2  |\n",
      "|2025-05-01 20:14:00|96562.0  |\n",
      "|2025-05-01 20:15:00|96604.0  |\n",
      "|2025-05-01 20:16:00|96607.0  |\n",
      "|2025-05-01 20:17:00|96607.0  |\n",
      "|2025-05-01 20:18:00|96627.8  |\n",
      "|2025-05-01 20:19:00|96625.0  |\n",
      "|2025-05-01 20:20:00|96625.0  |\n",
      "|2025-05-01 21:28:00|97106.0  |\n",
      "|2025-05-01 21:29:00|97106.0  |\n",
      "|2025-05-01 21:30:00|97093.0  |\n",
      "|2025-05-01 21:31:00|97061.2  |\n",
      "|2025-05-01 21:32:00|97054.0  |\n",
      "|2025-05-01 21:33:00|97027.0  |\n",
      "|2025-05-01 21:34:00|97007.0  |\n",
      "|2025-05-01 21:35:00|97007.0  |\n",
      "+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#Preprocess Data\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Extract the start timestamp from the window struct\n",
    "df_with_full_time = df.withColumn(\"timestamp\", col(\"window.start\"))\n",
    "\n",
    "# Show the results with the full time\n",
    "df_with_full_time.select(\"timestamp\", \"avg_price\").show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5acb6c1f-32cc-4546-9266-f35e06fd30ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 659:==================================================>    (11 + 1) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-----------------+\n",
      "|          timestamp|avg_price|       moving_avg|\n",
      "+-------------------+---------+-----------------+\n",
      "|2025-05-01 20:07:00|  96526.0|          96526.0|\n",
      "|2025-05-01 20:08:00|  96529.6|          96527.8|\n",
      "|2025-05-01 20:09:00|  96544.0|          96533.2|\n",
      "|2025-05-01 20:10:00|  96536.0|          96533.9|\n",
      "|2025-05-01 20:11:00|  96529.2|96532.95999999999|\n",
      "|2025-05-01 20:12:00|  96525.0|96531.63333333335|\n",
      "|2025-05-01 20:13:00|  96562.0|96537.63333333335|\n",
      "|2025-05-01 20:14:00|  96562.0|96543.03333333333|\n",
      "|2025-05-01 20:15:00|  96604.0|96553.03333333333|\n",
      "|2025-05-01 20:16:00|  96607.0|96564.86666666665|\n",
      "+-------------------+---------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#Time Series Analysis:\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.functions import avg, lag\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a moving average of Bitcoin prices to smooth out fluctuations\n",
    "window_spec = Window.orderBy(\"timestamp\").rowsBetween(-5, 0)\n",
    "\n",
    "df_clean = df_with_full_time.withColumn(\"moving_avg\", avg(col(\"avg_price\")).over(window_spec))\n",
    "\n",
    "# Show the results with moving average\n",
    "df_clean.select(\"timestamp\", \"avg_price\", \"moving_avg\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b290fc9e-eae1-43eb-9cf4-bf3978147c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 662:=====================================>                  (8 + 4) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+------------+\n",
      "|timestamp          |avg_price|lagged_price|\n",
      "+-------------------+---------+------------+\n",
      "|2025-05-01 20:07:00|96526.0  |null        |\n",
      "|2025-05-01 20:08:00|96529.6  |96526.0     |\n",
      "|2025-05-01 20:09:00|96544.0  |96529.6     |\n",
      "|2025-05-01 20:10:00|96536.0  |96544.0     |\n",
      "|2025-05-01 20:11:00|96529.2  |96536.0     |\n",
      "|2025-05-01 20:12:00|96525.0  |96529.2     |\n",
      "|2025-05-01 20:13:00|96562.0  |96525.0     |\n",
      "|2025-05-01 20:14:00|96562.0  |96562.0     |\n",
      "|2025-05-01 20:15:00|96604.0  |96562.0     |\n",
      "|2025-05-01 20:16:00|96607.0  |96604.0     |\n",
      "+-------------------+---------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#Create Features for Forecasting\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "# Define a new window spec for lag (just order by timestamp)\n",
    "lag_window = Window.orderBy(\"timestamp\")\n",
    "\n",
    "# Create lagged price column\n",
    "df_clean = df_clean.withColumn(\"lagged_price\", lag(\"avg_price\", 1).over(lag_window))\n",
    "\n",
    "# Display the data with lagged price\n",
    "df_clean.select(\"timestamp\", \"avg_price\", \"lagged_price\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e70d9-78cd-477c-973d-a5b1483cb2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
