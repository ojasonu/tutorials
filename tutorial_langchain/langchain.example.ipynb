{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b412cd",
   "metadata": {},
   "source": [
    "# Building a Documentation Chatbot with LangChain\n",
    "\n",
    "This script demonstrates how to build an intelligent chatbot that queries documentation using LangChain.\n",
    "The chatbot can:\n",
    "- Parse and preprocess Markdown files.\n",
    "- Embed document content for efficient similarity-based retrieval.\n",
    "- Answer detailed, context-aware queries from users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "532dfea4-93b4-41e2-b6c1-5d7f57a4140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo /venv/bin/pip install langchain --quiet\n",
    "#!sudo /venv/bin/pip install -U langchain-community --quiet\n",
    "#!sudo /venv/bin/pip install -U langchain-openai --quiet\n",
    "#!sudo /venv/bin/pip install -U langchain-core --quiet\n",
    "#!sudo /venv/bin/pip install -U langchainhub --quiet\n",
    "#!sudo /venv/bin/pip install -U unstructured python-magic pandoc markdown faiss-cpu --quiet\n",
    "#!sudo /venv/bin/pip install --quiet chromadb\n",
    "!sudo /venv/bin/pip install -U --quiet unstructured markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f52147f7-f5c1-460f-a110-5c708e569070",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cafda052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import helpers.hdbg as hdbg\n",
    "import langchain\n",
    "import langchain.chains\n",
    "import langchain.docstore.document as lngchdocstordoc\n",
    "import langchain.embeddings\n",
    "import langchain.hub\n",
    "import langchain.text_splitter\n",
    "import langchain_openai\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d76dc826-908c-4ecf-a5b9-c80b30f46256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "014b1dd4-f9ec-4263-8f53-657e4a7f64ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mWARNING: Running in Jupyter\n",
      "INFO  > cmd='/venv/lib/python3.12/site-packages/ipykernel_launcher.py -f /home/.local/share/jupyter/runtime/kernel-1e1769f8-d0de-4cbe-9738-0f57d8b46461.json'\n"
     ]
    }
   ],
   "source": [
    "hdbg.init_logger(verbosity=logging.INFO)\n",
    "\n",
    "_LOG = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a42c7e-203e-406d-a315-e5853fd2f000",
   "metadata": {},
   "source": [
    "## Define Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38a40064-5415-49c9-b2d5-354a839b9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # Define language model arguments.\n",
    "    \"language_model\": {\n",
    "        # Define your model here.\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"temperature\": 0,\n",
    "    },\n",
    "    # Define input directory path containing documents.\n",
    "    \"source_directory\": \"example_docs\",\n",
    "    \"parse_data_into_chunks\": {\n",
    "        \"chunk_size\": 500,\n",
    "        \"chunk_overlap\": 50,\n",
    "    },\n",
    "}\n",
    "\n",
    "hdbg.dassert_dir_exists(config[\"source_directory\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7208ea51-4419-4137-89a2-5f2756c14f1d",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "We'll begin by importing the required libraries and configuring the environment. The chatbot will use:\n",
    "- OpenAI's GPT-4o-mini as the core language model.\n",
    "- FAISS for fast document retrieval.\n",
    "- LangChain utilities for document parsing, text splitting, and chaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac548ed5-8093-43b0-a498-e17ebb61b0e0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt-4o-mini', 'temperature': 0}\n"
     ]
    }
   ],
   "source": [
    "# Set the OpenAI API key.\n",
    "#os.environ[\"OPENAI_API_KEY\"] = config[\"open_ai_api_key\"]\n",
    "# Initialize the chat model.\n",
    "print(config[\"language_model\"])\n",
    "chat_model = langchain_openai.ChatOpenAI(**config[\"language_model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c3a1e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e542cfc8",
   "metadata": {},
   "source": [
    "## Parse and Preprocess Documentation\n",
    "\n",
    "Markdown files serve as the primary data source for this chatbot.\n",
    "We'll parse the files into LangChain `Document` objects and split them into manageable chunks to ensure efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "305f9b0b-6739-4a5a-be60-db63b9def9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example_docs/README.md', 'example_docs/code_organization.md', 'example_docs/onboarding/ck.setup_vpn_and_dev_server_access.how_to_guide.md', 'example_docs/onboarding/admin.onboarding_process.reference.md', 'example_docs/onboarding/ck.hiring_process.how_to_guide.md', 'example_docs/onboarding/all.track_time_with_hubstaff.how_to_guide.md', 'example_docs/onboarding/ck.development_setup.how_to_guide.md', 'example_docs/onboarding/all.development_documents.reference.md', 'example_docs/onboarding/all.organize_email.how_to_guide.md', 'example_docs/onboarding/intern.set_up_development_on_laptop.how_to_guide.md', 'example_docs/onboarding/kaizenflow.prepare_for_development.how_to_guide.md', 'example_docs/onboarding/all.dev_must_read_checklist.reference.md', 'example_docs/onboarding/intern.onboarding_checklist.reference.md', 'example_docs/onboarding/all.onboarding_checklist.reference.md', 'example_docs/onboarding/kaizenflow.signing_up.how_to_guide.md', 'example_docs/onboarding/all.receive_crypto_payment.how_to_guide.md']\n"
     ]
    }
   ],
   "source": [
    "md_files = ut.list_markdown_files(config[\"source_directory\"])\n",
    "print(md_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5f91f41-afe7-49fa-9859-397009613558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  Found 16 markdown files in example_docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 37.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  Successfully parsed 16/16 files\n",
      "INFO  Split 16 documents into 184 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize with documents.\n",
    "md_files = ut.list_markdown_files(config[\"source_directory\"])\n",
    "raw_documents = ut.parse_markdown_files(md_files)\n",
    "chunked_documents = ut.split_documents(\n",
    "    raw_documents,\n",
    "    chunk_size=config[\"parse_data_into_chunks\"][\"chunk_size\"],\n",
    "    chunk_overlap=config[\"parse_data_into_chunks\"][\"chunk_overlap\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c896e-46aa-4d8f-b78e-a0f1c62e22e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15f1d16c-b0c5-4a1d-b733-ebf706849d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='File description\n",
      "\n",
      "Invariants:\n",
      "\n",
      "Files are organized by directory (e.g., docs, docs/work_tools)\n",
      "\n",
      "Each file name uses the Diataxis naming convention\n",
      "\n",
      "Each file name should be linked to the corresponding file as always\n",
      "\n",
      "Files are organized in alphabetical order to make it easy to add more files and see which file is missing\n",
      "\n",
      "Each file has a bullet lists summarizing its content using imperative mode\n",
      "\n",
      "In docs\n",
      "\n",
      "docs/README.md\n",
      "\n",
      "This file\n",
      "\n",
      "Describe all the available documentation files' metadata={'source': 'example_docs/README.md', 'last_modified': 1743723856.6785784, 'checksum': '96671db8a4b2d1d7be94bbcc56a5f965', 'start_index': 447}\n"
     ]
    }
   ],
   "source": [
    "print(chunked_documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9195c3-2278-4087-9003-0030b5d49365",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunked_documents[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31074294",
   "metadata": {},
   "source": [
    "## Create a FAISS Vector Store\n",
    "\n",
    "To enable fast document retrieval, we'll embed the document chunks using OpenAI's embeddings and store them in a FAISS vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63a3a326-140a-4543-8f01-155e0fa36192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO  Created vector store with 184 entries\n",
      "INFO  FAISS vector store created with 184 documents.\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI embeddings.\n",
    "embeddings = langchain.embeddings.OpenAIEmbeddings()\n",
    "# Create a FAISS vector store.\n",
    "vector_store = ut.create_vector_store(chunked_documents, embeddings)\n",
    "_LOG.info(\"FAISS vector store created with %d documents.\", len(chunked_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23fd80",
   "metadata": {},
   "source": [
    "## Build a QA Chain\n",
    "\n",
    "The `RetrievalQA` chain combines document retrieval with OpenAI's GPT-3.5 for question answering.\n",
    "It retrieves the most relevant document chunks and uses them as context to generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bab54142-1414-4d82-a6e1-1a51db624dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  Built retriever with config: {'k': 4}\n",
      "INFO  RetrievalQA chain initialized.\n"
     ]
    }
   ],
   "source": [
    "# Build the retriever from the vector store\n",
    "retriever = ut.build_retriever(vector_store)\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = langchain.chains.RetrievalQA.from_chain_type(\n",
    "    llm=chat_model, retriever=retriever, return_source_documents=True\n",
    ")\n",
    "\n",
    "_LOG.info(\"RetrievalQA chain initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e262aba",
   "metadata": {},
   "source": [
    "## Step 5: Query the Chatbot\n",
    "\n",
    "Let's interact with the chatbot! We'll ask it questions based on the documentation.\n",
    "The chatbot will retrieve relevant chunks and generate context-aware responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "427c7461-6d5c-4125-a2f4-2a57e5b62b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO  HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Answer:\n",
      "The provided context does not mention Diataxis beyond stating that file names use the Diataxis naming convention.\n",
      "\n",
      "Source Documents:\n",
      "- Source: example_docs/README.md\n",
      "  Excerpt: File description\n",
      "\n",
      "Invariants:\n",
      "\n",
      "Files are organized by directory (e.g., docs, docs/work_tools)\n",
      "\n",
      "Each file name uses the Diataxis naming convention\n",
      "\n",
      "Each file name should be linked to the corresponding \n",
      "- Source: example_docs/onboarding/kaizenflow.prepare_for_development.how_to_guide.md\n",
      "  Excerpt: Happy coding!\n",
      "\n",
      "Technologies used\n",
      "\n",
      "UMD DATA605 Big Data Systems contains lectures and tutorials about most of the technologies we use in KaizenFlow, e.g., Dask, Docker, Docker Compose, Git, github, Jup\n",
      "- Source: example_docs/onboarding/admin.onboarding_process.reference.md\n",
      "  Excerpt: Google form: ?\n",
      "\n",
      "CV / LinkedIn: ?\n",
      "\n",
      "GitHub handle: ?\n",
      "\n",
      "Email: ?\n",
      "\n",
      "Devops candidate: Yes / no\n",
      "\n",
      "Intern vs Full-time candidate: ?\n",
      "\n",
      "Quick review if the person has the skills we are looking for\n",
      "\n",
      "Assign to a sh\n",
      "- Source: example_docs/onboarding/all.dev_must_read_checklist.reference.md\n",
      "  Excerpt: You can, for example, grep for relevant keywords\n",
      "\n",
      "If the answer is not in the docs, reach out to your team leader, and, if it is deemed useful for other (future) team members, file a PR to update the \n"
     ]
    }
   ],
   "source": [
    "# Define a user query.\n",
    "#query = \"What are the guidelines for setting up a new project?\"\n",
    "query = \"Is there any mention of Diataxis?\"\n",
    "\n",
    "# Query the chatbot.\n",
    "response = qa_chain({\"query\": query})\n",
    "\n",
    "# Display the answer and source documents.\n",
    "print(f\"Answer:\\n{response['result']}\\n\")\n",
    "print(\"Source Documents:\")\n",
    "for doc in response[\"source_documents\"]:\n",
    "    print(f\"- Source: {doc.metadata['source']}\")\n",
    "    print(f\"  Excerpt: {doc.page_content[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d2ca4",
   "metadata": {},
   "source": [
    "## Step 6: Dynamic Updates\n",
    "\n",
    "What if the documentation changes? We'll handle this by monitoring the folder for new or modified files.\n",
    "The vector store will be updated dynamically to ensure the chatbot stays up-to-date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c959f-0def-4a38-b329-34bf7f3206bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.update_vector_store_from_changes(\n",
    "    config,\n",
    "    vector_store,\n",
    "    embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260331ce",
   "metadata": {},
   "source": [
    "## Step 7: Enhancements - Personalization\n",
    "\n",
    "We can extend the chatbot to include personalized responses:\n",
    "- Filter documents by metadata (e.g., tags, categories).\n",
    "- Customize responses based on user preferences.\n",
    "\n",
    "For example, users can ask for specific sections of the documentation or request summaries tailored to their needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e2de353-2f56-41e4-bf35-4d6c036bd539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO  HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO  HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Answer:\n",
      "I don't know.\n",
      "\n",
      "Source Documents:\n",
      "- Source: example_docs/onboarding/all.onboarding_checklist.reference.md\n",
      "  Excerpt: Onboarding Checklist\n",
      "\n",
      "Onboarding process for a new team member\n",
      "\n",
      "Meta\n",
      "\n",
      "Make on-boarding automatic\n",
      "\n",
      "Be patient\n",
      "\n",
      "Ask for confirmation\n",
      "\n",
      "Make on-boarding similar to our work routine\n",
      "\n",
      "Improve on-boarding pr\n",
      "- Source: example_docs/onboarding/all.onboarding_checklist.reference.md\n",
      "  Excerpt: Ask for confirmation of all the actions, e.g.,\n",
      "\n",
      "\"Does this and that work?\"\n",
      "\n",
      "\"Did you receive the email?\"\n",
      "\n",
      "\"Can you log in?\"\n",
      "\n",
      "Make the new team member follow the instructions so that they can get famil\n",
      "- Source: example_docs/onboarding/ck.hiring_process.how_to_guide.md\n",
      "  Excerpt: Follow the instructions in all.onboarding_checklist.reference.md\n",
      "\n",
      "HiringMeister: once the full onboarding is complete, organize more complex tasks to test their development and problem-solving skills\n",
      "\n",
      "- Source: example_docs/onboarding/all.onboarding_checklist.reference.md\n",
      "  Excerpt: We use it only to track the time automatically and as HR\n",
      "\n",
      "[ ] Team member:\n",
      "\n",
      "Confirm access to Hubstaff\n",
      "\n",
      "Read Instructions for Hubstaff\n",
      "\n",
      "IT setup\n",
      "\n",
      "[ ] Team leader: File an issue with this checklist\n",
      "\n",
      "Th\n"
     ]
    }
   ],
   "source": [
    "# Example query with personalized intent.\n",
    "personalized_query = \"Show me onboarding guidelines for new employees.\"\n",
    "\n",
    "# Query the chatbot.\n",
    "personalized_response = qa_chain({\"query\": personalized_query})\n",
    "\n",
    "# Display the personalized response.\n",
    "print(f\"Answer:\\n{personalized_response['result']}\\n\")\n",
    "print(\"Source Documents:\")\n",
    "for doc in personalized_response[\"source_documents\"]:\n",
    "    print(f\"- Source: {doc.metadata['source']}\")\n",
    "    print(f\"  Excerpt: {doc.page_content[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61e7b90",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this script, we:\n",
    "1. Parsed and processed Markdown documentation.\n",
    "2. Embedded document chunks into a FAISS vector store for efficient retrieval.\n",
    "3. Built a RetrievalQA chain for context-aware question answering.\n",
    "4. Enabled dynamic updates to handle changing documentation.\n",
    "5. Enhanced the chatbot with personalized query handling.\n",
    "\n",
    "This showcases how LangChain can be used to build intelligent, flexible chatbots tailored for specific tasks."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
